# Code Intelligence Reading List ðŸ“š

### The sections are as follows:
* [Survey and Tools](#survey-and-tools)
* [Benchmark and Evaluation](#benchmark-and-evaluation)
* [Code Pre-Training](#code-pre-training)
* [Related Tasks](#related-tasks)
    * [1. Code Search](#code-search)
    * [2. Code Completion](#code-completion)
    * [3. Code Translation](#code-translation)
    * [4. Code Generation](#code-generation)
    * [5. Code Summarization](#code-summarization)
* [Others](#others)
* [Hiring](#hiring)

## Survey and Tools

* Machine Learning on Source Code [[web](https://ml4code.github.io/)]

## Benchmark and Evaluation
This section covers datasets, evaluation methods on code related tasks.

* CodeSearchNet Challenge: Evaluating the State of Semantic Code Search (2020/06) [[CodeSearchNet](https://arxiv.org/pdf/1909.09436.pdf), [code](https://github.com/github/CodeSearchNet)]
* CodeBLEU: a Method for Automatic Evaluation of Code Synthesis (2020/09) [[CodeBLEU](https://arxiv.org/pdf/2009.10297.pdf)]
* CodeXGLUE: A Machine Learning Benchmark Dataset for Code Understanding and Generation (2021/03) [[CodeXGLUE](https://arxiv.org/pdf/2102.04664.pdf), [code](https://github.com/microsoft/CodeXGLUE)]
* Measuring Coding Challenge Competence With APPS (2021/05)[[APPS](https://arxiv.org/pdf/2105.09938.pdf), [code](https://github.com/hendrycks/apps)]
* CodeNet: A Large-Scale AI for Code Dataset for Learning a Diversity of Coding Tasks (2021/08) [[CodeNet](https://arxiv.org/pdf/2105.12655.pdf)]
* CoSQA: 20,000+ Web Queries for Code Search and Question Answering (2021/05, ACL) [[CoSQA](https://arxiv.org/pdf/2105.13239.pdf), [code](https://github.com/Jun-jie-Huang/CoCLR)]
* What Do They Capture? - A Structural Analysis of Pre-Trained Language Models for Source Code (2022/02) [[pdf](https://arxiv.org/pdf/2202.06840.pdf)]
* A Systematic Evaluation of Large Language Models of Code (2022/05) [[pdf](https://arxiv.org/pdf/2202.13169.pdf)]

## Code Pre-Training
* Learning and Evaluating Contextual Embedding of Source Code (2020/08, ICML) [[CuBERT](https://arxiv.org/pdf/2001.00059.pdf), [code](https://github.com/google-research/google-research/tree/master/cubert)]
* CodeBERT: A Pre-Trained Model for Programming and Natural Languages (2020/09, ACL) [[CodeBERT](https://arxiv.org/pdf/2002.08155.pdf), [code](https://github.com/microsoft/CodeBERT)]
* Evaluating Large Language Models Trained on Code (2021/07) [[CodeX](https://arxiv.org/pdf/2107.03374.pdf)]
* TreeBERT: A Tree-Based Pre-Trained Model for Programming Language (2021/07, UAI) [[TreeBERT](https://arxiv.org/pdf/2105.12485.pdf)]
* PLBART: Unified Pre-training for Program Understanding and Generation (2021/08, NAACL) [[PLBART](https://arxiv.org/pdf/2103.06333.pdf), [code](https://github.com/wasiahmad/PLBART)]
* CoTexT: Multi-task Learning with Code-Text Transformer (2021/06, ACL/NLP4Prog) [[CoTexT](https://arxiv.org/pdf/2105.08645.pdf), [code](https://github.com/justinphan3110/CoTexT)]
* SYNCOBERT: Syntax-Guided Multi-Modal Contrastive Pre-Training for Code Representation (2021/09, AAAI) [[SYNCOBERT](https://arxiv.org/pdf/2108.04556v3.pdf)]
* CodeT5: Identifier-aware Unified Pre-trained Encoder-Decoder Models
for Code Understanding and Generation (2021/06, ACL) [[CodeT5](https://arxiv.org/pdf/2109.00859.pdf), [code](https://github.com/salesforce/CodeT5)]
* DOBF: A Deobfuscation Pre-Training Objective for Programming Languages (2021/10, NeurIPS) [[DOBF](https://arxiv.org/pdf/2102.07492.pdf), [code](https://github.com/facebookresearch/CodeGens)]
* UniXcoder: Unified Cross-Modal Pre-training for Code Representation (2022/05, ACL) [[UniXcoder](https://arxiv.org/pdf/2203.03850.pdf), [code](https://github.com/microsoft/CodeBERT)]
* ContraCode: Contrastive Code Representation Learning (2021/11, EMNLP) [[ContraCode](https://arxiv.org/pdf/2007.04973.pdf), [code](https://github.com/parasj/contracode)]
* Text and Code Embeddings by Contrastive Pre-Training (2022/01) [[cpt-text](https://arxiv.org/pdf/2201.10005.pdf)]
* CODE-MVP: Learning to Represent Source Code from Multiple Views with Contrastive Pre-Training (2022/05) [[CODE-MVP](https://arxiv.org/pdf/2205.02029.pdf)]


## Related Tasks

### Code Search
* Towards Learning (Dis)-Similarity of Source Code from Program Contrasts (2022/03) [[pdf](https://arxiv.org/pdf/2110.03868.pdf)]

### Code Completion
* Multi-task Learning based Pre-trained Language Model for Code Completion (2020/12) [[pdf](https://arxiv.org/pdf/2012.14631.pdf)]
* ReACC: A Retrieval-Augmented Code Completion Framework (2022/03) [[pdf](https://arxiv.org/pdf/2203.07722.pdf), [code](https://github.com/microsoft/ReACC)]

### Code Translation
* TransCoder: Unsupervised Translation of Programming Languages (2020/09) [[pdf](https://arxiv.org/pdf/2006.03511.pdf),[code](https://github.com/facebookresearch/CodeGens)]
* TransCoder-ST: Leveraging Automated Unit Tests for Unsupervised Code Translation (2022/02) [[pdf](https://arxiv.org/pdf/2110.06773.pdf), [code](https://github.com/facebookresearch/CodeGens)]

### Code Generation
Todo

### Code Summarization
Todo

## Others


## Hiring

We are hiring interns in Shanghai office! If you are interested in working with us on NLP and large-scale pre-trained models on Code Intelligence, please send your resume to maoquanwang at microsoft.com.
