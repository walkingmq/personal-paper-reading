# Code Intelligence Reading List ðŸ“š

### The sections are as follows:
* [Survey and Tools](#survey-and-tools)
* [Benchmark and Evaluation](#benchmark-and-evaluation)
* [Code Pre-Training](#code-pre-training)
* [Related Tasks](#related-tasks)
    * [1. Code Search](#code-search)
    * [2. Code Completion](#code-completion)
    * [3. Code Translation](#code-translation)
    * [4. Code Generation](#code-generation)
    * [5. Code Summarization](#code-summarization)
* [Others](#others)

## Survey and Tools

* Machine Learning on Source Code [[web](https://ml4code.github.io/)]

## Benchmark and Evaluation
This section covers datasets, evaluation methods on code related tasks.

* CodeSearchNet Challenge: Evaluating the State of Semantic Code Search (2020/06) [[pdf](https://arxiv.org/pdf/1909.09436.pdf), [code](https://github.com/github/CodeSearchNet)]
* CodeBLEU: a Method for Automatic Evaluation of Code Synthesis (2020/09) [[pdf](https://arxiv.org/pdf/2009.10297.pdf)]
* CodeXGLUE: A Machine Learning Benchmark Dataset for Code Understanding and Generation (2021/03) [[pdf](https://arxiv.org/pdf/2102.04664.pdf), [code](https://github.com/microsoft/CodeXGLUE)]
* CodeNet: A Large-Scale AI for Code Dataset for Learning a Diversity of Coding Tasks (2021/08) [[pdf](https://arxiv.org/pdf/2105.12655.pdf), [code]()]
* What Do They Capture? - A Structural Analysis of Pre-Trained Language Models for Source Code (2022/02) [[pdf](https://arxiv.org/pdf/2202.06840.pdf), [code]()]
* A Systematic Evaluation of Large Language Models of Code (2022/05) [[pdf](https://arxiv.org/pdf/2202.13169.pdf), [code]()]

## Code Pre-Training
* CodeBERT: A Pre-Trained Model for Programming and Natural Languages (2020/09) [[pdf](https://arxiv.org/pdf/2002.08155.pdf), [code](https://github.com/microsoft/CodeBERT)]
* PLBART: Unified Pre-training for Program Understanding and Generation (2021/08) [[pdf](https://arxiv.org/pdf/2103.06333.pdf), [code](https://github.com/wasiahmad/PLBART)]
* CodeT5: Identifier-aware Unified Pre-trained Encoder-Decoder Models
for Code Understanding and Generation (2021/06) [[pdf](https://arxiv.org/pdf/2109.00859.pdf), [code](https://github.com/salesforce/CodeT5)]
* DOBF: A Deobfuscation Pre-Training Objective for Programming Languages (2021/10) [[pdf](https://arxiv.org/pdf/2102.07492.pdf), [code](https://github.com/facebookresearch/CodeGens)]

## Related Tasks

### Code Search
* Towards Learning (Dis)-Similarity of Source Code from Program Contrasts (2022/03) [[pdf](https://arxiv.org/pdf/2110.03868.pdf)]

### Code Completion
* ReACC: A Retrieval-Augmented Code Completion Framework (2022/03) [[pdf](https://arxiv.org/pdf/2203.07722.pdf), [code](https://github.com/microsoft/ReACC)]

### Code Translation
* TransCoder: Unsupervised Translation of Programming Languages (2020/09) [[pdf](https://arxiv.org/pdf/2006.03511.pdf),[code](https://github.com/facebookresearch/CodeGens)]
* TransCoder-ST: Leveraging Automated Unit Tests for Unsupervised Code Translation (2022/02) [[pdf](https://arxiv.org/pdf/2110.06773.pdf), [code](https://github.com/facebookresearch/CodeGens)]

### Code Generation
Todo

### Code Summarization
Todo

## Others


## Hiring

We are hiring interns in Shanghai office! If you are interested in working with us on NLP and large-scale pre-trained models on Code Intelligence, please send your resume to maoquanwang@microsoft.com.
